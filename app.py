import streamlit as st
import streamlit.components.v1 as components
import base64
from core.schemas import Node, Edge, Flowchart
from core.llm_client import LLMClient
from core.toon_parser import TOONParser
from core.history_mgr import HistoryManager
from core.exceptions import LLMAPIError, TOONParseError, FlowchartValidationError
from core.flow_extractor import FlowExtractor
from core.flow_merger import FlowMerger
from pathlib import Path

st.set_page_config(layout="wide")
st.title("Flowchart Generator & History Manager")

# 1. ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
history_mgr = HistoryManager()

# 2. ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ã®åˆæœŸåŒ–
if 'history' not in st.session_state:
    # åˆæœŸçŠ¶æ…‹ï¼šé–‹å§‹ã¨çµ‚äº†ã®ã¿ã®æ§‹æˆ
    initial_flow = Flowchart(
        nodes=[
            Node(id="start", label="é–‹å§‹", type="start"),
            Node(id="node_end", label="çµ‚äº†", type="end")
        ],
        edges=[Edge(source="start", target="node_end")]
    )
    st.session_state.history = [initial_flow]

# 3. è³ªå•å¿œç­”ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®åˆæœŸåŒ–
if 'conversation_context' not in st.session_state:
    st.session_state.conversation_context = None  # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
if 'pending_questions' not in st.session_state:
    st.session_state.pending_questions = None  # ç¾åœ¨ã®è³ªå•ãƒ†ã‚­ã‚¹ãƒˆ
if 'question_responses' not in st.session_state:
    st.session_state.question_responses = []  # å›ç­”ã®ãƒªã‚¹ãƒˆ
if 'append_mode_for_question' not in st.session_state:
    st.session_state.append_mode_for_question = False  # è³ªå•æ™‚ã®å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰è¨­å®š
if 'question_count' not in st.session_state:
    st.session_state.question_count = 0  # è³ªå•å›æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
if 'selected_node_ids' not in st.session_state:
    st.session_state.selected_node_ids = []  # é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰IDã®ãƒªã‚¹ãƒˆ
if 'selection_mode' not in st.session_state:
    st.session_state.selection_mode = 'text'  # 'text' or 'ui'
MAX_QUESTION_COUNT = 5  # è³ªå•å›æ•°ã®ä¸Šé™

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† ---
st.sidebar.header("ğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†")

# ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¸€è¦§ã‚’å–å¾—
saved_sessions = history_mgr.list_sessions()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åã®å…¥åŠ›ï¼ˆæ–°è¦ä½œæˆç”¨ï¼‰
session_name = st.sidebar.text_input("ã‚»ãƒƒã‚·ãƒ§ãƒ³åï¼ˆæ–°è¦ä½œæˆ/ä¿å­˜ç”¨ï¼‰", value="default_session")

col_save, col_load = st.sidebar.columns(2)
with col_save:
    if st.button("ä¿å­˜", use_container_width=True):
        history_mgr.save_session(session_name, st.session_state.history)
        # æœ€æ–°ã®Flowchartã‚’TOONå½¢å¼ã§ã‚‚ä¿å­˜
        if st.session_state.history:
            history_mgr.save_toon_file(session_name, st.session_state.history[-1])
        st.sidebar.success(f"'{session_name}' ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        st.rerun()

with col_load:
    if st.button("å‰Šé™¤", use_container_width=True):
        if session_name in saved_sessions:
            try:
                json_path = history_mgr.storage_dir / f"{session_name}.json"
                toon_path = history_mgr.toon_dir / f"{session_name}.md"
                if json_path.exists():
                    json_path.unlink()
                if toon_path.exists():
                    toon_path.unlink()
                st.sidebar.success(f"'{session_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                st.rerun()
            except Exception as e:
                st.sidebar.error(f"å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        else:
            st.sidebar.warning("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ï¼ˆé¸æŠå¼ï¼‰
st.sidebar.subheader("ğŸ“‚ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã‚€")
if saved_sessions:
    selected_session = st.sidebar.selectbox(
        "ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é¸æŠ",
        [""] + saved_sessions,
        key="session_selector"
    )
    if selected_session and st.sidebar.button("é¸æŠã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿", use_container_width=True):
        try:
            loaded_history = history_mgr.load_session(selected_session)
            if loaded_history:
                st.session_state.history = loaded_history
                st.sidebar.success(f"'{selected_session}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.rerun()
            else:
                st.sidebar.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        except ValueError as e:
            st.sidebar.error(str(e))
        except Exception as e:
            st.sidebar.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
else:
    st.sidebar.info("ä¿å­˜ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ã‚ã‚Šã¾ã›ã‚“")

# TOONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
st.sidebar.subheader("ğŸ“„ TOONãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†")
toon_files = history_mgr.list_toon_files()
if toon_files:
    selected_toon = st.sidebar.selectbox("TOONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", [""] + toon_files)
    if selected_toon and st.sidebar.button("TOONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"):
        try:
            loaded_flow = history_mgr.load_toon_file(selected_toon)
            if loaded_flow:
                st.session_state.history = [loaded_flow]
                st.sidebar.success(f"'{selected_toon}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.rerun()
            else:
                st.sidebar.error("TOONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except ValueError as e:
            st.sidebar.error(str(e))
else:
    st.sidebar.info("ä¿å­˜ã•ã‚ŒãŸTOONãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")

# --- ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šOllamaè¨­å®š ---
st.sidebar.divider()
st.sidebar.subheader("âš™ï¸ Ollamaè¨­å®š")
try:
    ollama_config = st.secrets.get("ollama", {})
    base_url = ollama_config.get("base_url", "http://localhost:11434")
    model = ollama_config.get("model", "llama3.2")
    config_source = "secrets.toml"
except (AttributeError, FileNotFoundError, ImportError, RuntimeError):
    import os
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model = os.getenv("OLLAMA_MODEL", "llama3.2")
    config_source = "ç’°å¢ƒå¤‰æ•°"

st.sidebar.text(f"è¨­å®šå…ƒ: {config_source}")
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ã€ãƒ™ãƒ¼ã‚¹URLã¯è¡¨ç¤ºã—ãªã„
st.sidebar.text(f"ãƒ¢ãƒ‡ãƒ«: {model}")

# --- æç”»ã¨å±¥æ­´ç®¡ç†ãƒ­ã‚¸ãƒƒã‚¯ ---
st.divider()

# å±¥æ­´ã‚’é¡ã‚‹ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
if len(st.session_state.history) > 1:
    history_index = st.sidebar.slider(
        "å±¥æ­´ã‚’é¡ã‚‹", 
        min_value=0, 
        max_value=len(st.session_state.history) - 1, 
        value=len(st.session_state.history) - 1
    )
else:
    history_index = 0
    st.sidebar.info("ç¾åœ¨ã¯åˆæœŸçŠ¶æ…‹ã§ã™ã€‚")

current_flow = st.session_state.history[history_index]

# 3ãƒšã‚¤ãƒ³æ§‹æˆ: å·¦ï¼ˆChat/Inputï¼‰ã€ä¸­å¤®ï¼ˆFlowchartï¼‰ã€å³ï¼ˆSourceï¼‰
col_chat, col_flow, col_source = st.columns([1, 2, 1])

with col_chat:
    st.subheader("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ")
    
    # è³ªå•ãŒä¿ç•™ä¸­ã®å ´åˆã¯è³ªå•ã‚’è¡¨ç¤ºã—ã€å›ç­”å…¥åŠ›æ¬„ã‚’è¡¨ç¤º
    if st.session_state.pending_questions:
        # è³ªå•ã¨å›ç­”ã®å±¥æ­´ã‚’è¡¨ç¤º
        if st.session_state.conversation_context:
            with st.expander("ğŸ“ ä¼šè©±å±¥æ­´", expanded=False):
                st.markdown("**å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:**")
                st.text(st.session_state.conversation_context)
                
                if st.session_state.question_responses:
                    st.markdown("**ã“ã‚Œã¾ã§ã®å›ç­”:**")
                    for i, response in enumerate(st.session_state.question_responses, 1):
                        st.markdown(f"{i}. {response}")
        
        st.info("ğŸ“‹ LLMã‹ã‚‰ã®è³ªå•:")
        st.markdown(st.session_state.pending_questions)
        
        st.divider()
        st.subheader("ğŸ’­ å›ç­”ã‚’å…¥åŠ›")
        
        # å›ç­”å…¥åŠ›æ¬„
        user_answer = st.text_area(
            "è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            placeholder="å›ç­”ã‚’å…¥åŠ›...",
            height=100,
            key="answer_input"
        )
        
        col_answer, col_cancel = st.columns([2, 1])
        with col_answer:
            if st.button("å›ç­”ã‚’é€ä¿¡", type="primary", use_container_width=True):
                if user_answer:
                    # å›ç­”ã‚’åé›†
                    st.session_state.question_responses.append(user_answer)
                    
                    # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€è³ªå•ã€å›ç­”ã‚’çµ„ã¿åˆã‚ã›ã¦å†é€ä¿¡
                    combined_prompt = st.session_state.conversation_context
                    if st.session_state.pending_questions:
                        combined_prompt += f"\n\nè³ªå•:\n{st.session_state.pending_questions}\n"
                    if st.session_state.question_responses:
                        combined_prompt += "\n\nå›ç­”:\n"
                        for i, response in enumerate(st.session_state.question_responses, 1):
                            combined_prompt += f"{i}. {response}\n"
                    
                    # è³ªå•æ™‚ã®å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’ä¿å­˜
                    append_mode_flag = st.session_state.append_mode_for_question
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã«å¾©å…ƒã™ã‚‹ãŸã‚ï¼‰
                    temp_context = st.session_state.conversation_context
                    temp_questions = st.session_state.pending_questions
                    temp_responses = st.session_state.question_responses.copy()
                    
                    # å†é€ä¿¡å‡¦ç†
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    status_text.info("LLMãŒãƒ•ãƒ­ãƒ¼ã‚’è¨­è¨ˆä¸­...ï¼ˆæœ€å¤§2åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
                    progress_bar.progress(10)
                    
                    try:
                        client = LLMClient()
                        raw_toon_text = client.generate_flow(combined_prompt, current_flow if append_mode_flag else None)
                        progress_bar.progress(100)
                        status_text.empty()
                        
                        # è³ªå•å½¢å¼ã®å¿œç­”ã‹ãƒã‚§ãƒƒã‚¯
                        if client.is_question_response(raw_toon_text):
                            # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                            st.session_state.question_count += 1
                            if st.session_state.question_count >= MAX_QUESTION_COUNT:
                                st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                                st.session_state.conversation_context = None
                                st.session_state.pending_questions = None
                                st.session_state.question_responses = []
                                st.session_state.question_count = 0
                            else:
                                # å†åº¦è³ªå•ãŒæ¥ãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
                                st.session_state.conversation_context = combined_prompt
                                st.session_state.pending_questions = raw_toon_text
                                st.session_state.question_responses = []  # æ–°ã—ã„è³ªå•ãªã®ã§å›ç­”ã‚’ãƒªã‚»ãƒƒãƒˆ
                                st.info(f"LLMã‹ã‚‰è¿½åŠ ã®è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                                st.rerun()
                        else:
                            # å‡ºåŠ›ã‚µã‚¤ã‚ºã®æ¤œè¨¼
                            is_valid, validation_message = client.validate_output_size(raw_toon_text)
                            if not is_valid:
                                st.warning(validation_message)
                                st.info("ä¸»è¦ãªãƒ«ãƒ¼ãƒˆã®ã¿ã‚’ç”Ÿæˆã™ã‚‹ã‹ã€ãƒ•ãƒ­ãƒ¼ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
                            
                            # TOONå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹æˆåŠŸæ™‚ã®ã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                            st.session_state.conversation_context = None
                            st.session_state.pending_questions = None
                            st.session_state.question_responses = []
                            st.session_state.question_count = 0  # è³ªå•å›æ•°ã‚‚ãƒªã‚»ãƒƒãƒˆ
                            
                            # TOONå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹
                            new_flow = TOONParser.parse(raw_toon_text)
                            
                            # è«–ç†ã®ç©´æ¤œçŸ¥ã‚’é©ç”¨
                            new_flow = new_flow.apply_logic_gap_detection()
                            
                            # å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
                            if append_mode_flag:
                                merged_flow = history_mgr.append_toon_log(session_name, new_flow)
                                st.session_state.history.append(merged_flow)
                                st.success(f"'{session_name}' ã®TOONãƒ•ã‚¡ã‚¤ãƒ«ã«å·®åˆ†ã‚’è¿½è¨˜ã—ã¾ã—ãŸ")
                            else:
                                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šå±¥æ­´ã«è¿½åŠ 
                                st.session_state.history.append(new_flow)
                        
                        st.rerun()
                    except LLMAPIError as e:
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å¾©å…ƒ
                        st.session_state.conversation_context = temp_context
                        st.session_state.pending_questions = temp_questions
                        st.session_state.question_responses = temp_responses
                        st.error(f"LLM APIã‚¨ãƒ©ãƒ¼: {e}")
                        st.info("OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                    except TOONParseError as e:
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å¾©å…ƒ
                        st.session_state.conversation_context = temp_context
                        st.session_state.pending_questions = temp_questions
                        st.session_state.question_responses = temp_responses
                        
                        # è³ªå•å½¢å¼ã®å¿œç­”ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                        if 'raw_toon_text' in locals() and raw_toon_text:
                            if client.is_question_response(raw_toon_text):
                                # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                                st.session_state.question_count += 1
                                if st.session_state.question_count >= MAX_QUESTION_COUNT:
                                    st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                                    st.session_state.conversation_context = None
                                    st.session_state.pending_questions = None
                                    st.session_state.question_responses = []
                                    st.session_state.question_count = 0
                                else:
                                    # è³ªå•å½¢å¼ã®å¿œç­”ã ã£ãŸå ´åˆ
                                    st.session_state.conversation_context = combined_prompt
                                    st.session_state.pending_questions = raw_toon_text
                                    st.session_state.question_responses = []
                                    st.info(f"LLMã‹ã‚‰è¿½åŠ ã®è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                                    st.rerun()
                            else:
                                st.error(f"TOONå½¢å¼ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                                with st.expander("ğŸ” LLMã®ç”Ÿå‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", expanded=True):
                                    st.info("ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’TOONå½¢å¼ã¨ã—ã¦è§£é‡ˆã—ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€å¤±æ•—ã—ã¾ã—ãŸã€‚")
                                    st.code(raw_toon_text)
                        else:
                            st.error(f"TOONå½¢å¼ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                    except Exception as e:
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å¾©å…ƒ
                        st.session_state.conversation_context = temp_context
                        st.session_state.pending_questions = temp_questions
                        st.session_state.question_responses = temp_responses
                        
                        # è³ªå•å½¢å¼ã®å¿œç­”ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                        if 'raw_toon_text' in locals() and raw_toon_text:
                            try:
                                client = LLMClient()
                                if client.is_question_response(raw_toon_text):
                                    # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                                    st.session_state.question_count += 1
                                    if st.session_state.question_count >= MAX_QUESTION_COUNT:
                                        st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                                        st.session_state.conversation_context = None
                                        st.session_state.pending_questions = None
                                        st.session_state.question_responses = []
                                        st.session_state.question_count = 0
                                    else:
                                        st.session_state.conversation_context = combined_prompt
                                        st.session_state.pending_questions = raw_toon_text
                                        st.session_state.question_responses = []
                                        st.info(f"LLMã‹ã‚‰è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                                        st.rerun()
                            except:
                                pass
                        
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
                else:
                    st.warning("å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        with col_cancel:
            if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True):
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                st.session_state.conversation_context = None
                st.session_state.pending_questions = None
                st.session_state.question_responses = []
                st.session_state.question_count = 0  # è³ªå•å›æ•°ã‚‚ãƒªã‚»ãƒƒãƒˆ
                st.rerun()
        
        st.divider()
    
    # å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    append_mode = st.checkbox(
        "æ—¢å­˜TOONãƒ•ã‚¡ã‚¤ãƒ«ã«å·®åˆ†è¿½è¨˜ï¼ˆLOGï¼‰",
        help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã€æ—¢å­˜ã®TOONãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ã—ã¾ã™ã€‚"
    )
    
    # éƒ¨åˆ†ãƒ•ãƒ­ãƒ¼ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    partial_mode = st.checkbox(
        "éƒ¨åˆ†ãƒ•ãƒ­ãƒ¼ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰",
        value=True,
        help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã€ä¸»è¦ãªãƒ«ãƒ¼ãƒˆã®ã¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚å…¨ãƒ«ãƒ¼ãƒˆã‚’å«ã‚€è¤‡é›‘ãªãƒ•ãƒ­ãƒ¼ã¯é¿ã‘ã¾ã™ã€‚"
    )
    
    user_prompt = st.text_area(
        "ã©ã®ã‚ˆã†ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ãŸã„ã§ã™ã‹ï¼Ÿ", 
        placeholder="ä¾‹ï¼šãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®æ•…éšœè¨ºæ–­æ‰‹é †ã‚’ãƒ•ãƒ­ãƒ¼ã«ã—ã¦",
        help="è‡ªç„¶è¨€èªã§å…¥åŠ›ã™ã‚‹ã¨LLMãŒTOONå½¢å¼ã‚’ç”Ÿæˆã—ã€ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¾ã™ã€‚",
        height=150
    )
    
    if st.button("ãƒ•ãƒ­ãƒ¼ã‚’ç”Ÿæˆ", type="primary", use_container_width=True):
        if user_prompt:
            progress_bar = st.progress(0)
            status_text = st.empty()
            status_text.info("LLMãŒãƒ•ãƒ­ãƒ¼ã‚’è¨­è¨ˆä¸­...ï¼ˆæœ€å¤§2åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
            progress_bar.progress(10)
            
            raw_toon_text = ""
            try:
                # LLMã¨ã®é€šä¿¡
                client = LLMClient()
                # å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æ—¢å­˜ã®Flowchartã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ¸¡ã™
                context_flowchart = current_flow if append_mode else None
                
                # éƒ¨åˆ†ãƒ•ãƒ­ãƒ¼ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ æŒ‡ç¤ºã‚’ä»˜ä¸
                enhanced_prompt = user_prompt
                if partial_mode:
                    enhanced_prompt = user_prompt + "\n\nã€é‡è¦ã€‘ä¸»è¦ãªãƒ«ãƒ¼ãƒˆã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å…¨ãƒ«ãƒ¼ãƒˆã‚’å«ã‚€å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ¼ãƒ‰æ•°ã¯30å€‹ä»¥ä¸‹ã€ã‚¨ãƒƒã‚¸æ•°ã¯50å€‹ä»¥ä¸‹ã«ã—ã¦ãã ã•ã„ã€‚"
                
                raw_toon_text = client.generate_flow(enhanced_prompt, context_flowchart)
                progress_bar.progress(100)
                status_text.empty()
                
                # è³ªå•å½¢å¼ã®å¿œç­”ã‹ãƒã‚§ãƒƒã‚¯
                if client.is_question_response(raw_toon_text):
                    # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                    st.session_state.question_count += 1
                    if st.session_state.question_count >= MAX_QUESTION_COUNT:
                        st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                        st.session_state.conversation_context = None
                        st.session_state.pending_questions = None
                        st.session_state.question_responses = []
                        st.session_state.question_count = 0
                    else:
                        # è³ªå•å½¢å¼ã®å¿œç­”ã®å ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                        st.session_state.conversation_context = user_prompt
                        st.session_state.pending_questions = raw_toon_text
                        st.session_state.question_responses = []
                        st.session_state.append_mode_for_question = append_mode  # å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’ä¿å­˜
                        st.info(f"LLMã‹ã‚‰è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                        st.rerun()
                else:
                    # å‡ºåŠ›ã‚µã‚¤ã‚ºã®æ¤œè¨¼
                    is_valid, validation_message = client.validate_output_size(raw_toon_text)
                    if not is_valid:
                        st.warning(validation_message)
                        st.info("ä¸»è¦ãªãƒ«ãƒ¼ãƒˆã®ã¿ã‚’ç”Ÿæˆã™ã‚‹ã‹ã€ãƒ•ãƒ­ãƒ¼ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
                    
                    # TOONå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹
                    new_flow = TOONParser.parse(raw_toon_text)
                    
                    # è«–ç†ã®ç©´æ¤œçŸ¥ã‚’é©ç”¨
                    new_flow = new_flow.apply_logic_gap_detection()
                    
                    # å·®åˆ†è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
                    if append_mode:
                        merged_flow = history_mgr.append_toon_log(session_name, new_flow)
                        st.session_state.history.append(merged_flow)
                        st.success(f"'{session_name}' ã®TOONãƒ•ã‚¡ã‚¤ãƒ«ã«å·®åˆ†ã‚’è¿½è¨˜ã—ã¾ã—ãŸ")
                    else:
                        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šå±¥æ­´ã«è¿½åŠ 
                        st.session_state.history.append(new_flow)
                    
                    # æˆåŠŸæ™‚ã®ã¿è³ªå•å›æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
                    st.session_state.question_count = 0
                    st.rerun()
            except LLMAPIError as e:
                # LLM APIã‚¨ãƒ©ãƒ¼
                st.error(f"LLM APIã‚¨ãƒ©ãƒ¼: {e}")
                st.info("OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            except TOONParseError as e:
                # TOONå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼
                # è³ªå•å½¢å¼ã®å¿œç­”ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if 'raw_toon_text' in locals() and raw_toon_text:
                    client = LLMClient()
                    if client.is_question_response(raw_toon_text):
                        # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                        st.session_state.question_count += 1
                        if st.session_state.question_count >= MAX_QUESTION_COUNT:
                            st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                            st.session_state.conversation_context = None
                            st.session_state.pending_questions = None
                            st.session_state.question_responses = []
                            st.session_state.question_count = 0
                        else:
                            # è³ªå•å½¢å¼ã®å¿œç­”ã ã£ãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                            st.session_state.conversation_context = user_prompt
                            st.session_state.pending_questions = raw_toon_text
                            st.session_state.question_responses = []
                            st.info(f"LLMã‹ã‚‰è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                            st.rerun()
                    else:
                        # æœ¬å½“ã«ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                        st.error(f"TOONå½¢å¼ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                        with st.expander("ğŸ” LLMã®ç”Ÿå‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", expanded=True):
                            st.info("ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’TOONå½¢å¼ã¨ã—ã¦è§£é‡ˆã—ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€å¤±æ•—ã—ã¾ã—ãŸã€‚")
                            st.code(raw_toon_text)
                else:
                    st.error(f"TOONå½¢å¼ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            except FlowchartValidationError as e:
                # Flowchartãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ï¼ˆè­¦å‘Šã¨ã—ã¦è¡¨ç¤ºã€è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œï¼‰
                st.warning(f"ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®æ¤œè¨¼ã§å•é¡Œã‚’æ¤œå‡ºã—ã¾ã—ãŸ: {e}")
                st.info("è«–ç†ã®ç©´æ¤œçŸ¥ã§è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œã—ã¾ã™ã€‚")
                # è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œï¼ˆæ—¢ã«apply_logic_gap_detectionãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã€å†åº¦è©¦è¡Œï¼‰
                try:
                    if 'new_flow' in locals():
                        corrected_flow = new_flow.apply_logic_gap_detection()
                        st.session_state.history.append(corrected_flow)
                        st.session_state.question_count = 0  # æˆåŠŸæ™‚ã¯è³ªå•å›æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
                        st.success("è‡ªå‹•ä¿®æ­£ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                        st.rerun()
                except Exception as correction_error:
                    st.error(f"è‡ªå‹•ä¿®æ­£ã«å¤±æ•—ã—ã¾ã—ãŸ: {correction_error}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            except ValueError as e:
                # ãã®ä»–ã®ValueErrorï¼ˆãƒ¢ãƒ‡ãƒ«å¿œç­”ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            except Exception as e:
                # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
                st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¨ãƒ©ãƒ¼æ™‚ã«ç”Ÿå‡ºåŠ›ã‚’ç¢ºèªã§ãã‚‹ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã‚’è¡¨ç¤º
                if 'raw_toon_text' in locals() and raw_toon_text:
                    # è³ªå•å½¢å¼ã®å¿œç­”ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    try:
                        client = LLMClient()
                        if client.is_question_response(raw_toon_text):
                            # è³ªå•å›æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
                            st.session_state.question_count += 1
                            if st.session_state.question_count >= MAX_QUESTION_COUNT:
                                st.error(f"è³ªå•å›æ•°ãŒä¸Šé™ï¼ˆ{MAX_QUESTION_COUNT}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                                st.session_state.conversation_context = None
                                st.session_state.pending_questions = None
                                st.session_state.question_responses = []
                                st.session_state.question_count = 0
                            else:
                                # è³ªå•å½¢å¼ã®å¿œç­”ã ã£ãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                                st.session_state.conversation_context = user_prompt
                                st.session_state.pending_questions = raw_toon_text
                                st.session_state.question_responses = []
                                st.info(f"LLMã‹ã‚‰è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè³ªå•å›æ•°: {st.session_state.question_count}/{MAX_QUESTION_COUNT}ï¼‰")
                                st.rerun()
                    except:
                        pass
                    
                    with st.expander("ğŸ” LLMã®ç”Ÿå‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", expanded=True):
                        st.info("ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’TOONå½¢å¼ã¨ã—ã¦è§£é‡ˆã—ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.code(raw_toon_text)
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯st.rerun()ã‚’å‘¼ã°ãªã„ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
        else:
            st.warning("æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

with col_flow:
    st.subheader("ğŸ“Š ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ")
    # Mermaidæç”»å‡¦ç†
    mermaid_code = current_flow.to_mermaid()
    b64_mermaid = base64.b64encode(mermaid_code.encode('utf-8')).decode('utf-8')

    html_path = Path("frontend/index.html")
    if html_path.exists():
        html_content = html_path.read_text(encoding='utf-8')
        # ãƒãƒ¼ãƒ‰é¸æŠç”¨ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ç”¨ï¼‰
        components.html(
            f"""
            {html_content}
            <script>
                window.onload = () => {{
                    window.postMessage({{ 
                        type: "render", 
                        base64Code: "{b64_mermaid}" 
                    }}, "*");
                }};
                
                // Streamlitã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã‚’è¨­å®š
                window.addEventListener('message', function(event) {{
                    // ãƒãƒ¼ãƒ‰é¸æŠãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Streamlitã«é€ä¿¡
                    if (event.data && event.data.type === 'node_selected') {{
                        // Streamlitã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«å¾“ã£ã¦é€ä¿¡
                        window.parent.postMessage({{
                            type: 'streamlit:setComponentValue',
                            value: event.data.nodeId
                        }}, '*');
                    }}
                }});
            </script>
            """,
            height=800,
            scrolling=True
        )
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ï¼ˆStreamlitã®ä»•æ§˜ã«å¾“ã†ï¼‰
        # æ³¨æ„: components.htmlã¯ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã§ããªã„ãŸã‚ã€
        # JavaScriptã‹ã‚‰é€ä¿¡ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹åˆ¥ã®æ–¹æ³•ã‚’ä½¿ç”¨
    else:
        st.error("frontend/index.html ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

with col_source:
    st.subheader("ğŸ“„ TOONå½¢å¼ï¼ˆSourceï¼‰")
    
    # TOONå½¢å¼ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    toon_text = current_flow.to_toon_format()
    
    with st.expander("TOONå½¢å¼ã‚’è¡¨ç¤º", expanded=True):
        st.code(toon_text, language="markdown")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    st.download_button(
        label="ğŸ“¥ TOONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=toon_text,
        file_name=f"{session_name}_toon.md",
        mime="text/markdown"
    )
    
    # ãƒãƒ¼ãƒ‰é¸æŠã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.divider()
    st.subheader("ğŸ¯ ãƒãƒ¼ãƒ‰é¸æŠï¼ˆéƒ¨åˆ†å¤‰æ›´ç”¨ï¼‰")
    
    # é¸æŠãƒ¢ãƒ¼ãƒ‰
    selection_mode = st.radio(
        "é¸æŠæ–¹æ³•",
        ["UIé¸æŠ", "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ä¸¡æ–¹"],
        horizontal=True,
        key="selection_mode_radio"
    )
    
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    if selection_mode in ["ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ä¸¡æ–¹"]:
        node_ids_input = st.text_input(
            "ãƒãƒ¼ãƒ‰IDï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
            placeholder="ä¾‹: task1, task2, decision1",
            help="å¤‰æ›´ã—ãŸã„ãƒãƒ¼ãƒ‰ã®IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚startã¨endã¯é¸æŠã§ãã¾ã›ã‚“ã€‚",
            key="node_ids_input"
        )
        
        if node_ids_input:
            selected_ids = [id.strip() for id in node_ids_input.split(",")]
            # startã¨endã‚’é™¤å¤–
            selected_ids = [id for id in selected_ids 
                          if id not in ["start", "node_end"]]
            # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            valid_ids = [id for id in selected_ids 
                        if id in [n.id for n in current_flow.nodes]]
            invalid_ids = [id for id in selected_ids if id not in valid_ids]
            
            if invalid_ids:
                st.warning(f"å­˜åœ¨ã—ãªã„ãƒãƒ¼ãƒ‰ID: {invalid_ids}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆæ—¢å­˜ã®é¸æŠã«è¿½åŠ ï¼‰
            if 'selected_node_ids' not in st.session_state:
                st.session_state.selected_node_ids = []
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã§æŒ‡å®šã•ã‚ŒãŸIDã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
            for node_id in valid_ids:
                if node_id not in st.session_state.selected_node_ids:
                    st.session_state.selected_node_ids.append(node_id)
    
    # UIé¸æŠã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
    # Streamlitã®components.htmlã¯ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã§ããªã„ãŸã‚ã€
    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ¼ãƒ‰é¸æŠã‚’å‡¦ç†
    query_params = st.query_params
    if 'selected_node' in query_params:
        selected_node_id = query_params['selected_node']
        if selected_node_id and selected_node_id not in ["start", "node_end"]:
            if selected_node_id in [n.id for n in current_flow.nodes]:
                if 'selected_node_ids' not in st.session_state:
                    st.session_state.selected_node_ids = []
                if selected_node_id not in st.session_state.selected_node_ids:
                    st.session_state.selected_node_ids.append(selected_node_id)
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†èª­ã¿è¾¼ã¿ã‚’é˜²ã
        st.query_params.clear()
        if 'selected_node' in query_params:
            st.rerun()
    
    # ãƒãƒ¼ãƒ‰ä¸€è¦§ã‹ã‚‰é¸æŠï¼ˆUIé¸æŠã®ä»£æ›¿æ–¹æ³•ï¼‰
    if selection_mode in ["UIé¸æŠ", "ä¸¡æ–¹"]:
        st.markdown("**ãƒãƒ¼ãƒ‰ä¸€è¦§ã‹ã‚‰é¸æŠ:**")
        # ç¾åœ¨ã®ãƒ•ãƒ­ãƒ¼ã®ãƒãƒ¼ãƒ‰ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆstartã¨endã‚’é™¤ãï¼‰
        available_nodes = [n for n in current_flow.nodes 
                          if n.id not in ["start", "node_end"]]
        
        if available_nodes:
            # ãƒãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
            node_dict = {f"{n.id} ({n.label})": n.id for n in available_nodes}
            selected_node_labels = st.multiselect(
                "ãƒãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰",
                options=list(node_dict.keys()),
                default=[label for label, node_id in node_dict.items() 
                        if node_id in st.session_state.get('selected_node_ids', [])],
                key="node_multiselect"
            )
            
            # é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰IDã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åæ˜ 
            if 'selected_node_ids' not in st.session_state:
                st.session_state.selected_node_ids = []
            
            # é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰IDã‚’æ›´æ–°
            selected_node_ids_from_ui = [node_dict[label] for label in selected_node_labels]
            # æ—¢å­˜ã®é¸æŠã¨ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
            current_selected = set(st.session_state.selected_node_ids)
            new_selected = set(selected_node_ids_from_ui)
            st.session_state.selected_node_ids = list(current_selected | new_selected)
        else:
            st.info("é¸æŠå¯èƒ½ãªãƒãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®è¡¨ç¤º
    if 'selected_node_ids' in st.session_state and st.session_state.selected_node_ids:
        st.info(f"é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰: {', '.join(st.session_state.selected_node_ids)}")
        
        # é¸æŠè§£é™¤ãƒœã‚¿ãƒ³
        col_clear1, col_clear2 = st.columns(2)
        with col_clear1:
            if st.button("é¸æŠã‚’ã‚¯ãƒªã‚¢", key="clear_selection"):
                st.session_state.selected_node_ids = []
                st.rerun()
        with col_clear2:
            if st.button("æœ€å¾Œã®é¸æŠã‚’å‰Šé™¤", key="remove_last_selection"):
                if st.session_state.selected_node_ids:
                    st.session_state.selected_node_ids.pop()
                st.rerun()
        
        # éƒ¨åˆ†å¤‰æ›´ã®å…¥åŠ›
        change_instruction = st.text_area(
            "å¤‰æ›´æŒ‡ç¤º",
            placeholder="ä¾‹: ã“ã®ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ã‚’ã€Œç¢ºèªå‡¦ç†ã€ã«å¤‰æ›´ã—ã¦ãã ã•ã„",
            height=100,
            key="change_instruction"
        )
        
        if st.button("é¸æŠç¯„å›²ã‚’å¤‰æ›´", type="primary", key="apply_partial_change"):
            if not change_instruction.strip():
                st.error("å¤‰æ›´æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            else:
                try:
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    status_text.info("é¸æŠç¯„å›²ã‚’å¤‰æ›´ä¸­...ï¼ˆæœ€å¤§2åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
                    progress_bar.progress(10)
                    
                    # 1. é¸æŠç¯„å›²ã‚’æŠ½å‡º
                    selected_node_ids = st.session_state.selected_node_ids
                    partial_flowchart = FlowExtractor.extract_node_range(
                        current_flow, 
                        selected_node_ids
                    )
                    progress_bar.progress(30)
                    
                    if not partial_flowchart.nodes:
                        st.error("é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        progress_bar.empty()
                        status_text.empty()
                    else:
                        # 2. LLMã«éƒ¨åˆ†å¤‰æ›´ã‚’ä¾é ¼
                        client = LLMClient()
                        status_text.info("LLMãŒå¤‰æ›´ã‚’ç”Ÿæˆä¸­...")
                        progress_bar.progress(50)
                        changed_toon = client.generate_partial_change(
                            change_instruction,
                            partial_flowchart,
                            current_flow
                        )
                        progress_bar.progress(70)
                        
                        # 3. å¤‰æ›´çµæœã‚’ãƒ‘ãƒ¼ã‚¹
                        status_text.info("å¤‰æ›´çµæœã‚’è§£æä¸­...")
                        parser = TOONParser()
                        changed_partial = parser.parse(changed_toon)
                        progress_bar.progress(85)
                        
                        # 4. å…¨ä½“ãƒ•ãƒ­ãƒ¼ã«ãƒãƒ¼ã‚¸
                        status_text.info("ãƒ•ãƒ­ãƒ¼ã‚’ãƒãƒ¼ã‚¸ä¸­...")
                        merged_flowchart = FlowMerger.merge_partial_change(
                            current_flow,
                            changed_partial,
                            selected_node_ids
                        )
                        progress_bar.progress(95)
                        
                        # 5. å±¥æ­´ã«è¿½åŠ 
                        history_mgr.append_toon_log(
                            session_name,
                            merged_flowchart
                        )
                        
                        # 6. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
                        st.session_state.history = history_mgr.load_history(session_name)
                        st.session_state.selected_node_ids = []  # é¸æŠã‚’ã‚¯ãƒªã‚¢
                        
                        progress_bar.progress(100)
                        status_text.empty()
                        progress_bar.empty()
                        st.success("é¸æŠç¯„å›²ã®å¤‰æ›´ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        st.rerun()
                            
                except LLMAPIError as e:
                    st.error(f"LLM APIã‚¨ãƒ©ãƒ¼: {str(e)}")
                except TOONParseError as e:
                    st.error(f"TOONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
                except FlowchartValidationError as e:
                    st.error(f"ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                except Exception as e:
                    st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())

with st.sidebar.expander("ç¾åœ¨ã®Mermaidã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º"):
    st.code(mermaid_code)